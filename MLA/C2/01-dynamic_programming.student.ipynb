{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b313b45f",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Outlook\n",
    "\n",
    "In this colab we will investigate the **value iteration** and **policy\n",
    "iteration** algorithms in a maze environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db51dd44",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aba3c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs the necessary Python and system libraries\n",
    "try:\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "except ModuleNotFoundError as e:\n",
    "    get_ipython().run_line_magic(\"pip\", \"install 'easypip>=1.2.0'\")\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "\n",
    "easyinstall(\"swig\")\n",
    "easyinstall(\"bbrl>=0.2.0\")\n",
    "easyinstall(\"bbrl_gymnasium>=0.2.0\")\n",
    "easyinstall(\"tensorboard\")\n",
    "easyinstall(\"moviepy\")\n",
    "easyinstall(\"box2d-kengz\")\n",
    "\n",
    "from moviepy.editor import ipython_display as video_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd717551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "if is_notebook():\n",
    "    get_ipython().run_line_magic(\"matplotlib\", \"inline\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "easyimport(\"bbrl_gymnasium\")\n",
    "easyimport(\"moviepy\")\n",
    "from bbrl_gymnasium.envs.maze_mdp import MazeMDPEnv\n",
    "\n",
    "# For visualization\n",
    "if not os.path.isdir(\"./videos\"):\n",
    "    os.mkdir(\"./videos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64684ac",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "# Agents and MDPs\n",
    "\n",
    "A reinforcement learning agent interacts with an environment represented as a\n",
    "Markov Decision Process (MDP). It is defined by a tuple $(S, A, P, r, \\gamma)$\n",
    "where $S$ is the state space, $A$ is the action space, $P(state_t, action_t,\n",
    "state_{t+1})$ is the transition function, $r(state_t, action_t)$ is the reward\n",
    "function and $\\gamma \\in [0, 1]$ is the discount factor.\n",
    "\n",
    "In what follows we import code to create an MDP corresponding to a random maze\n",
    "(see https://github.com/osigaud/SimpleMazeMDP for documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d9390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import bbrl_gymnasium\n",
    "\n",
    "env = gym.make(\"MazeMDP-v0\", kwargs={\"width\": 5, \"height\": 5, \"ratio\": 0.2},render_mode=\"rgb_array\")\n",
    "env.metadata['render_fps'] = 1\n",
    "env.reset()\n",
    "\n",
    "# in dynamic programming, there is no agent moving in the environment\n",
    "env.set_no_agent()\n",
    "env.init_draw(\"The maze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30750933",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "# Dynamic programming\n",
    "\n",
    "The goal of an RL agent is to find the optimal behaviour, defined by a policy\n",
    "$\\pi$ that assigns an action (or distribution over actions) to each state so\n",
    "as to maximize the agent's total expected reward. In order to estimate how\n",
    "good a state is, either a state value function $V(x)$ or a state-action value\n",
    "function $Q(x,u)$ is used.\n",
    "\n",
    "Dynamic programming algorithms are used for planning, they require a full\n",
    "knowledge of the MDP from the agent (in contrast to \"true\" RL where the agent\n",
    "does not know the transition and reward functions). They find the optimal\n",
    "policy by computing a value function $V$ or an action-value function $Q$ over\n",
    "the state space or state-action space of the given MDP. **Value iteration**\n",
    "and **policy iteration** are two standard dynamic programming algorithms. You\n",
    "should study both of them using both $V$ and $Q$, as these algorithms contain\n",
    "the basic building blocks for most RL algorithms.\n",
    "\n",
    "## Value Iteration ##\n",
    "\n",
    "### Value Iteration with the V function ###\n",
    "\n",
    "When using the $V$ function, **value iteration** aims at finding the optimal\n",
    "values $V^*$ based on the Bellman Optimality Equation: $$V^*(s) = \\max_a \\big[\n",
    "r(s,a) + \\gamma \\sum_{y \\in S} P(s,a,y)V^*(y) \\big],$$\n",
    "\n",
    "where:\n",
    "\n",
    "*   $r(s, a)$ is the reward obtained from taking action $a$ in state $s$,\n",
    "*   $P(s, a, y)$ is the probability of reaching state $y$ when taking action\n",
    "    $a$ in state $s$, \n",
    "*   $\\gamma \\in [0,1]$ is a discount factor defining the relative importance\n",
    "    of long term rewards over short term ones (the closer to 0, the more the\n",
    "    agent focuses on immediate rewards).\n",
    "\n",
    "In practice, we start with an initial value function $V^0$ (for instance, the\n",
    "values of all states are 0), and then we iterate for all states $s$\n",
    "\n",
    "$$V^{i+1}(s) = \\max_a \\big[ r(s,a) + \\gamma \\sum_{y \\in S} P(s,a,y)V^i(y)\n",
    "\\big],$$\n",
    "\n",
    "until the values converge, that is $\\forall s, V^{i+1}(s) \\approx V^i(s)$. It\n",
    "is shown that at convergence, $\\forall s, V^i(s)= V^*(s)$.\n",
    "\n",
    "To visualize the policy obtained from **value iteration**, we need to first\n",
    "define the `get_policy_from_V()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4f8755",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_policy_from_v(mdp: MazeMDPEnv, v: np.ndarray) -> np.ndarray:\n",
    "    # Outputs a policy given the state values\n",
    "    policy = np.zeros(mdp.nb_states)  # initial state values are set to 0\n",
    "    for x in range(mdp.nb_states):  # for each state x\n",
    "        if x in mdp.terminal_states:\n",
    "            policy[x] = np.argmax(mdp.r[x, :])\n",
    "        else:\n",
    "            # Compute the value of the state x for each action u of the MDP action space\n",
    "            v_temp = []\n",
    "            for u in range(mdp.action_space.n):\n",
    "                # Process sum of the values of the neighbouring states\n",
    "                summ = 0\n",
    "                for y in range(mdp.nb_states):\n",
    "                    summ = summ + mdp.P[x, u, y] * v[y]\n",
    "                v_temp.append(mdp.r[x, u] + mdp.gamma * summ)\n",
    "            policy[x] = np.argmax(v_temp)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d72813",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 2
   },
   "source": [
    "The `value_iteration_v(mdp)` function below provides the code of **value\n",
    "iteration** using the $V$ function. It is given as an example from which you\n",
    "can derive other instances of dynamic programming algorithms. Look at it more\n",
    "closely, this will help for later questions:\n",
    "\n",
    "* you can ignore the `mdp.new_render()` and `mdp.render(...)` functions which\n",
    "  are here to provide the visualization of the iterations.\n",
    "* find in the code the loop over states, the main loop that performs these\n",
    "  updates until the values don't change significantly anymore, the main update\n",
    "  equation. Found them? OK, you can continue..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8edf15",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from gymnasium.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "def value_iteration_v(mdp: MazeMDPEnv, render: bool = True) -> Tuple[np.ndarray, List[float]]:\n",
    "    # Value Iteration using the state value v\n",
    "    v = np.zeros(mdp.nb_states)  # initial state values are set to 0\n",
    "    v_list = []\n",
    "    stop = False\n",
    "\n",
    "    video_recorder = VideoRecorder(mdp, \"videos/ValueIterationV.mp4\", enabled=render)\n",
    "    mdp.init_draw(\"Value iteration V\", recorder=video_recorder)\n",
    "    \n",
    "    mdp.draw_v(v, recorder=video_recorder)\n",
    "\n",
    "    while not stop:\n",
    "        v_old = v.copy()\n",
    "        mdp.draw_v(v, recorder=video_recorder)\n",
    "\n",
    "        for x in range(mdp.nb_states):  # for each state x\n",
    "            # Compute the value of the state x for each action u of the MDP action space\n",
    "            if x in mdp.terminal_states:\n",
    "                v[x] = np.max(mdp.r[x, :])\n",
    "            else:\n",
    "                v_temp = []\n",
    "                for u in range(mdp.action_space.n):\n",
    "                    # Process sum of the values of the neighbouring states\n",
    "                    summ = 0\n",
    "                    for y in range(mdp.nb_states):\n",
    "                        summ = summ + mdp.P[x, u, y] * v_old[y]\n",
    "                    v_temp.append(mdp.r[x, u] + mdp.gamma * summ)\n",
    "\n",
    "                # Select the highest state value among those computed\n",
    "                v[x] = np.max(v_temp)\n",
    "\n",
    "        # Test if convergence has been reached\n",
    "        if (np.linalg.norm(v - v_old)) < 0.01:\n",
    "            stop = True\n",
    "        v_list.append(np.linalg.norm(v))\n",
    "\n",
    "    policy = get_policy_from_v(mdp, v)\n",
    "    mdp.draw_v_pi(v, policy, recorder=video_recorder)\n",
    "    video_recorder.close()\n",
    "    return v, v_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f059c73",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Let us run it on the previously defined MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5128717",
   "metadata": {},
   "outputs": [],
   "source": [
    "v, v_list = value_iteration_v(env, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7818eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_display(\"./videos/ValueIterationV.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aea06f8",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "### Value iteration with the $Q$ function ###\n",
    "\n",
    "The state-action value function $Q^{\\pi}(s,a)$ defines the value of being in\n",
    "state $s$, taking action $a$ then following policy $\\pi$. The Bellman\n",
    "Optimality Equation for $Q^*$ is $$ Q^*(s,a) =  r(s,a) + \\gamma \\sum_{y}\n",
    "P(s,a,y) \\max_{a'}Q^*(y,a'). $$ \n",
    "\n",
    "**Question:** By taking inspiration from the `value_iteration_v(mdp)` function\n",
    "above, fill the blank (given with '\\#Q[x,u]=...') in the code of\n",
    "`value_iteration_q(mdp)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc579551",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ------------------ Value Iteration with the Q function ---------------------#\n",
    "# Given a MDP, this algorithm computes the optimal action value function Q\n",
    "# It then derives the optimal policy based on this function\n",
    "\n",
    "def value_iteration_q(mdp: MazeMDPEnv, render: bool = True) -> Tuple[np.ndarray, List[float]]:\n",
    "    q = np.zeros((mdp.nb_states, mdp.action_space.n))  # initial action values are set to 0\n",
    "    q_list = []\n",
    "    stop = False\n",
    "\n",
    "    video_recorder = VideoRecorder(mdp, \"videos/ValueIterationQ.mp4\", enabled=render)\n",
    "    mdp.init_draw(\"Value iteration Q\", recorder=video_recorder)\n",
    "    \n",
    "    mdp.draw_v(q, recorder=video_recorder)\n",
    "\n",
    "    while not stop:\n",
    "        qold = q.copy()\n",
    "\n",
    "        mdp.draw_v(q, recorder=video_recorder)\n",
    "\n",
    "        for x in range(mdp.nb_states):\n",
    "            for u in range(mdp.action_space.n):\n",
    "                if x in mdp.terminal_states:\n",
    "                    q[x, u] = mdp.r[x, u]\n",
    "                else:\n",
    "                    summ = 0\n",
    "                    for y in range(mdp.nb_states):\n",
    "                        summ += mdp.P[x, u, y] * np.max(qold[y, :])\n",
    "                        \n",
    "                    # Compléter d'après la formule ci-dessus\n",
    "\n",
    "                    q[x, u] = ...\n",
    "                    assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "        if (np.linalg.norm(q - qold)) <= 0.01:\n",
    "            stop = True\n",
    "        q_list.append(np.linalg.norm(q))\n",
    "\n",
    "    mdp.draw_v(q, recorder=video_recorder)\n",
    "    video_recorder.close()\n",
    "\n",
    "    return q, q_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2f3f3d",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Once you are done, run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bc8aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "q, q_list = value_iteration_q(env, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84743ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_display(\"videos/ValueIterationQ.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e26580c",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Policy Iteration ##\n",
    "\n",
    "The **policy iteration** algorithm is more complicated than **value iteration**.\n",
    "Given a MDP and a policy $\\pi$, **policy iteration** iterates the following\n",
    "steps: \n",
    "\n",
    "*   Evaluate policy $\\pi$: compute $V$ or $Q$ based on the policy $\\pi$;\n",
    "*   Improve policy $\\pi$: compute a better policy based on $V$ or $Q$.\n",
    "\n",
    "This process is repeated until convergence, i.e. when the policy cannot be\n",
    "improved anymore.\n",
    "\n",
    "### Policy iteration with the $V$ function ###\n",
    "\n",
    "When using $V$, $V^{\\pi}(s)$ is the expected return when starting from state\n",
    "$s$ and following policy $\\pi$. It is processed based on the Bellman\n",
    "Optimality Equation for deterministic policies:\n",
    "\n",
    "$$V^\\pi(s) = r(s, \\pi(s)) + \\gamma \\sum_{y \\in S}P(s, \\pi(s), y)V^\\pi(y),$$\n",
    "\n",
    "where:\n",
    "\n",
    "*   $\\pi$ is a deterministic policy, meaning that in a state $s$, the agent\n",
    "    always selects the same action,\n",
    "*   $V^\\pi(y)$ is the value of the state $y$ under policy $\\pi$.\n",
    "\n",
    "Thus, given a policy $\\pi$, one must first compute its value function\n",
    "$V^\\pi(s)$ for all states $s$ iterating the Bellman Optimality Equation until\n",
    "convergence, that is using **value iteration**. Then, one must determine if\n",
    "policy $\\pi$ can be improved based on $V$. For that, in each state $s$, one\n",
    "can compute the Q-value $Q(s,a)$ of applying action $a$ and then following\n",
    "policy $\\pi$ based on the just computed $V^\\pi$, and replace the action\n",
    "$\\pi(s)$ with $\\arg\\max_a Q(s,a)$.\n",
    "\n",
    "In order to facilitate the coding of **policy iteration** algorithms, we first\n",
    "define a set of useful functions.\n",
    "\n",
    "The `improve_policy_from_v(mdp, v, policy)` function is very similar to the\n",
    "`get_policy_from_v(v)` function which was given above. The main difference is\n",
    "that it takes a policy as argument and improves this policy when possible,\n",
    "thus is is more in the spirit of the `policy improvement` step of **policy\n",
    "iteration**. But both functions can be used interchangeably.\n",
    "\n",
    "The functions `evaluate_one_step_v(mdp, v, policy)`, where `mdp` is a given\n",
    "MDP, `v` is some value function in this MDP and `policy` is some policy and\n",
    "the function `evaluate_v(mdp, policy)` are also given. These functions are\n",
    "used to build the value function $V^\\pi$ corresponding to policy $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d703f790",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def improve_policy_from_v(mdp: MazeMDPEnv, v: np.ndarray, policy: np.ndarray) -> np.ndarray:\n",
    "    # Improves a policy given the state values\n",
    "    for x in range(mdp.nb_states):  # for each state x\n",
    "        # Compute the value of the state x for each action u of the MDP action space\n",
    "        if x in mdp.terminal_states:\n",
    "            policy[x] = np.argmax(mdp.r[x, :])\n",
    "        else:\n",
    "            v_temp = np.zeros(mdp.action_space.n)\n",
    "            for u in range(mdp.action_space.n):\n",
    "                # Process sum of the values of the neighbouring states\n",
    "                summ = 0\n",
    "                for y in range(mdp.nb_states):\n",
    "                    summ = summ + mdp.P[x, u, y] * v[y]\n",
    "                v_temp[u] = mdp.r[x, u] + mdp.gamma * summ\n",
    "\n",
    "            for u in range(mdp.action_space.n):\n",
    "                if v_temp[u] > v_temp[policy[x]]:\n",
    "                    policy[x] = u\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52da3800",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_one_step_v(mdp: MazeMDPEnv, v: np.ndarray, policy: np.ndarray) -> np.ndarray:\n",
    "    # Outputs the state value function after one step of policy evaluation\n",
    "    # Corresponds to one application of the Bellman Operator\n",
    "    v_new = np.zeros(mdp.nb_states)  # initial state values are set to 0\n",
    "    for x in range(mdp.nb_states):  # for each state x\n",
    "        # Compute the value of the state x for each action u of the MDP action space\n",
    "        if x in mdp.terminal_states:\n",
    "            v_new[x] = mdp.r[x, policy[x]]\n",
    "        else:\n",
    "            # Process sum of the values of the neighbouring states\n",
    "            summ = 0\n",
    "            for y in range(mdp.nb_states):\n",
    "                summ = summ + mdp.P[x, policy[x], y] * v[y]\n",
    "            v_new[x] = mdp.r[x, policy[x]] + mdp.gamma * summ\n",
    "    return v_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cff19ff",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_v(mdp: MazeMDPEnv, policy: np.ndarray) -> np.ndarray:\n",
    "    # Outputs the state value function of a policy\n",
    "    v = np.zeros(mdp.nb_states)  # initial state values are set to 0\n",
    "    stop = False\n",
    "    while not stop:\n",
    "        vold = v.copy()\n",
    "        v = evaluate_one_step_v(mdp, vold, policy)\n",
    "\n",
    "        # Test if convergence has been reached\n",
    "        if (np.linalg.norm(v - vold)) < 0.01:\n",
    "            stop = True\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb55beb",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "To perform **policy iteration** we also need an initial random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70842c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabularmazemdp import random_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac76f69",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "**Question:** By using the above functions, fill the code of the\n",
    "`policy_iteration_v(mdp)` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f086698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Policy Iteration with the V function -----------------#\n",
    "# Given an MDP, this algorithm simultaneously computes \n",
    "# the optimal state value function V and the optimal policy\n",
    "\n",
    "def policy_iteration_v(mdp: MazeMDPEnv, render: bool = True) -> Tuple[np.ndarray, List[float]]:\n",
    "    # policy iteration over the v function\n",
    "    v = np.zeros(mdp.nb_states)  # initial state values are set to 0\n",
    "    v_list = []\n",
    "    policy = random_policy(mdp)\n",
    "\n",
    "    stop = False\n",
    "\n",
    "    video_recorder = VideoRecorder(mdp, \"videos/PolicyIterationV.mp4\", enabled=render)\n",
    "    mdp.init_draw(\"Policy Iteration (V)\", recorder=video_recorder)\n",
    "\n",
    "    while not stop:\n",
    "        vold = v.copy()\n",
    "\n",
    "        mdp.draw_v(v, title=\"Policy iteration Q\", recorder=video_recorder)\n",
    "\n",
    "        # Step 1 : Policy Evaluation\n",
    "        # To be completed...\n",
    "\n",
    "        v = ...\n",
    "        assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "\n",
    "        # Step 2 : Policy Improvement\n",
    "        # To be completed...\n",
    "\n",
    "        policy = ...\n",
    "        assert False, 'Not implemented yet'\n",
    "\n",
    "        \n",
    "        # Check convergence\n",
    "        if (np.linalg.norm(v - vold)) < 0.01:\n",
    "            stop = True\n",
    "        v_list.append(np.linalg.norm(v))\n",
    "\n",
    "    mdp.draw_v_pi(v, get_policy_from_v(mdp, v), recorder=video_recorder)\n",
    "    video_recorder.close()\n",
    "\n",
    "    return v, v_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b177f2d",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "And finally run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be446295",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "v, v_list = policy_iteration_v(env, render=True)\n",
    "video_display(\"videos/PolicyIterationV.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2318b5c4",
   "metadata": {
    "cell_marker": "r\"\"\"",
    "lines_to_next_cell": 2
   },
   "source": [
    "### Policy iteration with the $Q$ function ###\n",
    "\n",
    "The **policy iteration** algorithm with the $Q$ function is the same as with\n",
    "the $V$ function, but the policy improvement step is more straightforward.\n",
    "\n",
    "When using $Q$, the Bellman Optimality Equation with deterministic policy\n",
    "$\\pi$ for $Q$ becomes: $$Q^{\\pi}(s,a) = r(s,a) + \\gamma \\sum_{y \\in\n",
    "S}P(s,a,y)Q^{\\pi}(y,\\pi(y)).$$\n",
    "\n",
    "The policy can then be updated as follows: $$\\pi^{(t+1)}(s) =\n",
    "\\arg\\max_aQ^{\\pi^{(t)}}(s,a).$$\n",
    "\n",
    "First, we need to determine a policy from the $Q$ function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec13ab14",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "**Question:**  fill the `get_policy_from_q(q)` function, where $q$ is the\n",
    "state-action value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1f86e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy_from_q(q: np.ndarray) -> np.ndarray:\n",
    "    # Outputs a policy given the action values\n",
    "\n",
    "    return ...\n",
    "    assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da114459",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "**Question:** By drawing inspiration on the functions give with the $v$\n",
    "function, fill the code of the `evaluate_one_step_q(mdp, q, policy)` function\n",
    "below, where $q$ is some action value function, and the `evaluate_q(mdp,\n",
    "policy)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9094f15",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_one_step_q(mdp: MazeMDPEnv, q: np.ndarray, policy: np.ndarray) -> np.ndarray:\n",
    "    # Outputs the state value function after one step of policy evaluation\n",
    "    qnew = np.zeros((mdp.nb_states, mdp.action_space.n))  # initial action values are set to 0\n",
    "    for x in range(mdp.nb_states):  # for each state x\n",
    "        # Compute the value of the state x for each action u of the MDP action space\n",
    "        for u in range(mdp.action_space.n):\n",
    "            if x in mdp.terminal_states:\n",
    "                qnew[x, u] = mdp.r[x, u]\n",
    "            else:\n",
    "                # Process sum of the values of the neighbouring states\n",
    "                summ = 0\n",
    "                for y in range(mdp.nb_states):\n",
    "                    # To be completed...\n",
    "\n",
    "                    summ += ...\n",
    "                    assert False, 'Not implemented yet'\n",
    "\n",
    "                    \n",
    "                # To be completed...\n",
    "\n",
    "                qnew[x, u] = ...\n",
    "                assert False, 'Not implemented yet'\n",
    "\n",
    "    return qnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70e98b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_q(mdp: MazeMDPEnv, policy: np.ndarray) -> np.ndarray:\n",
    "    # Outputs the state value function of a policy\n",
    "    q = np.zeros((mdp.nb_states, mdp.action_space.n))  # initial action values are set to 0\n",
    "    stop = False\n",
    "    while not stop:\n",
    "        qold = q.copy()\n",
    "        \n",
    "        # To be completed...\n",
    "\n",
    "        q = ...\n",
    "        assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "        # Test if convergence has been reached\n",
    "        if (np.linalg.norm(q - qold)) < 0.01:\n",
    "            stop = True\n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a2fb4d",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "**Question:** By using the above functions, fill the code of the\n",
    "`policy_iteration_q(mdp)` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209d3b13",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ---------------- Policy Iteration with the Q function -----------------#\n",
    "# Given a MDP, this algorithm simultaneously computes \n",
    "# the optimal action value function Q and the optimal policy\n",
    "\n",
    "def policy_iteration_q(mdp: MazeMDPEnv, render: bool = True) -> Tuple[np.ndarray, List[float]]:\n",
    "    \"\"\"policy iteration over the q function.\"\"\"\n",
    "    q = np.zeros((mdp.nb_states, mdp.action_space.n))  # initial action values are set to 0\n",
    "    q_list = []\n",
    "    policy = random_policy(mdp)\n",
    "\n",
    "    stop = False\n",
    "\n",
    "    video_recorder = VideoRecorder(mdp, \"videos/PolicyIterationQ.mp4\", enabled=render)\n",
    "    mdp.init_draw(\"Policy iteration Q\", recorder=video_recorder)\n",
    "\n",
    "    while not stop:\n",
    "        qold = q.copy()\n",
    "        mdp.draw_v(q, recorder=video_recorder)\n",
    "\n",
    "        # Step 1 : Policy evaluation\n",
    "        # To be completed...\n",
    "\n",
    "        q = ...\n",
    "        assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "        # Step 2 : Policy improvement\n",
    "        # To be completed...\n",
    "\n",
    "        policy = ...\n",
    "        assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "        # Check convergence\n",
    "        if (np.linalg.norm(q - qold)) <= 0.01:\n",
    "            stop = True\n",
    "        q_list.append(np.linalg.norm(q))\n",
    "\n",
    "    mdp.draw_v_pi(q, get_policy_from_q(q), recorder=video_recorder)\n",
    "    video_recorder.close()\n",
    "    return q, q_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e4c897",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Finally, run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec97d9c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "q, q_list = policy_iteration_q(env, render=True)\n",
    "video_display(\"videos/PolicyIterationQ.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a308a349",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Study part: Experimental comparisons\n",
    "\n",
    "We will now compare the efficiency of the various dynamic programming methods\n",
    "using either the $V$ or the  $Q$ functions.\n",
    "\n",
    "In all your dymanic programming functions, add code to count the number of\n",
    "iterations and the number of elementary $V$ or $Q$ updates. Use the provided\n",
    "`mazemdp.Chrono` class to measure the time taken. You may generate various\n",
    "mazes of various sizes to figure out the influence of the maze topology.\n",
    "\n",
    "Build a table where you compare the various dymanic programming functions in\n",
    "terms of iterations, elementary operations and time taken.\n",
    "\n",
    "You can run the `plot_convergence_vi_pi(...)` function provided below to\n",
    "visualize the convergence of the various algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa0fdf2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ---- plot learning curves of Q-Learning and Sarsa using epsilon-greedy and softmax -----#\n",
    "\n",
    "def plot_convergence_vi_pi(m, render):\n",
    "    v, v_list1 = value_iteration_v(m, render)\n",
    "    q, q_list1 = value_iteration_q(m, render)\n",
    "    v, v_list2 = policy_iteration_v(m, render)\n",
    "    q, q_list2 = policy_iteration_q(m, render)\n",
    "\n",
    "    plt.plot(range(len(v_list1)), v_list1, label='value_iteration_v')\n",
    "    plt.plot(range(len(q_list1)), q_list1, label='value_iteration_q')\n",
    "    plt.plot(range(len(v_list2)), v_list2, label='policy_iteration_v')\n",
    "    plt.plot(range(len(q_list2)), q_list2, label='policy_iteration_q')\n",
    "\n",
    "    plt.xlabel('Number of episodes')\n",
    "    plt.ylabel('Norm of V or Q value')\n",
    "    plt.legend(loc='upper right')\n",
    "    # plt.savefig(\"comparison_DP.png\")\n",
    "    plt.title(\"Comparison of convergence rates\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4c8baa",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "**Question:** Run the code below and visualize the results of the different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd2a038",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence_vi_pi(env, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a4c66b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Given the results above, discuss the relative computational\n",
    "efficiency of these methods.\n",
    "\n",
    "### Study part: Generalized Policy Iteration\n",
    "\n",
    "Code the **generalized policy iteration** algorithm and study the influence of\n",
    "the number of evaluation steps between each improvement step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb93f8b8",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "# Generalized Policy Iteration\n",
    "\n",
    "## Generalized Policy Iteration with the Q function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6bb000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one_step_q_count(mdp: MazeMDPEnv, q: np.ndarray, policy: np.ndarray) -> np.ndarray:\n",
    "    # Outputs the state value function after one step of policy evaluation\n",
    "    # To be completed...\n",
    "\n",
    "    ...\n",
    "    assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa287bb6",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "A difference with respect to the previous version is that now we pass the number of iterations K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4180bac",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_q_K_steps(mdp: MazeMDPEnv, policy: np.ndarray, q: np.array, K: int) -> np.ndarray:\n",
    "    # Outputs the state value function of a policy\n",
    "    # To be completed...\n",
    "\n",
    "    assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0a2501",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generalized_policy_iteration_q(mdp: MazeMDPEnv, render: bool = True, K: int = 1) -> Tuple[np.ndarray, List[float]]:\n",
    "    \"\"\"policy iteration over the q function.\"\"\"\n",
    "    # To be completed...\n",
    "\n",
    "    assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6fab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "q, q_list = generalized_policy_iteration_q(env, render=True, K=1)\n",
    "video_display(\"videos/GeneralizedPolicyIterationQ.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec5537",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Generalized Policy Iteration with the V function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d6d3fa",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_one_step_v(mdp: MazeMDPEnv, v: np.ndarray, policy: np.ndarray) -> np.ndarray:\n",
    "    # Outputs the state value function after one step of policy evaluation\n",
    "    # Corresponds to one application of the Bellman Operator\n",
    "    # To be completed...\n",
    "\n",
    "    assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82109692",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "A difference with respect to the previous version is that now we pass the number of iterations K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4974a409",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_v_count(mdp: MazeMDPEnv, policy: np.ndarray, v: np.array, norms: np.array, K: int, threshold: float) -> np.ndarray:\n",
    "    # Outputs the state value function of a policy\n",
    "    # To be completed...\n",
    "\n",
    "    assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3191b3fe",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generalized_policy_iteration_v(mdp: MazeMDPEnv, K: int = 1, threshold: float = 0.01, render: bool = True) -> Tuple[np.ndarray, List[float]]:\n",
    "    \"\"\"policy iteration over the q function.\"\"\"\n",
    "    # To be completed...\n",
    "\n",
    "    assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e52de3d",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Visualization of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad4f8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "max = 10\n",
    "threshold=0.00001\n",
    "for k in range(max):\n",
    "  norms = np.zeros(max)\n",
    "  v, norm = generalized_policy_iteration_v(env, k, threshold, render=False)\n",
    "  # print(k, \":\", norm)\n",
    "  plt.plot(range(len(norm)), norm, label=k)\n",
    "  plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
