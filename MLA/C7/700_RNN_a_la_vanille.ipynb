{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/fr/8/81/Sciences_SU.png\" width=\"240\" height=\"240\" align=\"center\"/>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLA 700. RNN à la vanille [régression de série temporelle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dans ce notebook, nous allons nous intéresser à une introduction aux RNN sur une tâche de régression\n",
    "# sur des séries temporelles\n",
    "\n",
    "# Ce notebook vise à apprendre : \n",
    "# - L'organisation des données de séries temporelles numériques pour l'apprentissage d'un RNN\n",
    "# - L'implémentation d'un RNN à la vanille en TF \n",
    "# - L'implémentation exhaustive d'une RNN pour l'inférence en Python\n",
    "# - Leur comparison numérique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On importe les librairies de base pour le calcul et l'affichage\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# On crée X périodes d'un sinus de fréquence 10 Hz sur 1000 points\n",
    "n_samp = 1000\n",
    "f = 10\n",
    "tmin = 0\n",
    "tmax = 5\n",
    "t = np.linspace(tmin,tmax,n_samp)\n",
    "x = np.sin(2*math.pi*f*t)\n",
    "xt=x\n",
    "\n",
    "# On trace le sinus\n",
    "plt.plot(t,x)\n",
    "plt.xlabel(\"Temps [en s.]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On formate les données pour l'entrée du RNN \n",
    "# 3 dimensions dans un RNN : (nbatch, ntime, ndim)\n",
    "\n",
    "# En entrée du RNN, on utilise les len_seq valeurs précédentes pour prédire la valeur courante\n",
    "# Dimension de l'entrée : (nbatch, len_seq-1, 1)\n",
    "# Dimension de la sortie : (nbatch, 1)\n",
    "# On créé autant de batch que nécessaire en découpant les données par paquets de len_seq\n",
    "len_seq = 10\n",
    "dataX, dataY = [], []\n",
    "for i in range(len(x)-len_seq):\n",
    "    dataX.append([x[i:i+len_seq]])\n",
    "    dataY.append([x[i+1:i+len_seq+1]])    \n",
    "dataX = np.array(dataX)\n",
    "dataY = np.array(dataY)\n",
    "\n",
    "# On formate dataX pour que ses dimensions soient bien (nbatch, len_seq, 1)\n",
    "x     = np.reshape(x, (1, n_samp, 1))\n",
    "dataX = np.reshape(dataX, (dataX.shape[0], dataX.shape[2], dataX.shape[1]))\n",
    "dataY = np.reshape(dataY, (dataY.shape[0], dataY.shape[2], dataY.shape[1]))\n",
    "\n",
    "# On vérifie le format des données d'entrée et de sortie\n",
    "print(\"Dimension des données en entrée : {}\" .format(dataX.shape))\n",
    "print(\"Dimension des données en sortie : {}\" .format(dataY.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2 : apprendre ce réseau en utilisant TF\n",
    "# On va comparer 3 configurations :\n",
    "# 1) hid_dim=4 et stateful=False\n",
    "# 2) hid_dim=4 et stateful=True\n",
    "# 3) hid_dim=16 et stateful=True\n",
    "\n",
    "# on importe les librairies de TF\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "\n",
    "# On créé quelques paramètres d'entrée du réseau\n",
    "batch_size = 10\n",
    "hid_dim    = 16  \n",
    "epochs     = 10\n",
    "n_dim      = 1\n",
    "\n",
    "# On instancie le réseau\n",
    "RNN_model  = Sequential(name=\"RNN_model\")\n",
    "RNN_model.add(SimpleRNN(units=hid_dim, batch_input_shape=(batch_size, len_seq, n_dim), stateful=True, return_sequences=True, activation='tanh', name='RNN'))\n",
    "RNN_model.add(TimeDistributed(Dense(1), name='dense'))\n",
    "\n",
    "# On compile le réseau\n",
    "RNN_model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# On visualise la structure du réseau\n",
    "RNN_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On entraine le réseau\n",
    "RNN_model.fit(dataX, dataY, epochs=epochs, batch_size=batch_size, verbose=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# On récupère les poids du RNN \n",
    "# (avec la méthode get_weights de layer)\n",
    "w = {}\n",
    "for layer in RNN_model.layers:\n",
    "    w[layer.name] = layer.get_weights()\n",
    "\n",
    "# On définit une fonction qui réalise explicitement une passe de RNN pour une valeur donnée en entrée\n",
    "def RNN_predict(w,x,h=0):\n",
    "    '''\n",
    "    Entrées:\n",
    "    w : poids du RNN \n",
    "    x : valeur d'entrée au temps t\n",
    "    h : valeur de l'état caché au temps t [def:0]\n",
    "    Sorties:\n",
    "    y : valeur de sortie au temps t\n",
    "    '''\n",
    "    \n",
    "    # calcul de h à l'instant t à partir de xt, h_{t-1}, et des poids (Wh, Uh, bh)\n",
    "    Wx    = np.transpose(w[\"RNN\"][0])\n",
    "    Wh    = np.transpose(w[\"RNN\"][1])\n",
    "    bh    = np.transpose(np.reshape(w[\"RNN\"][2], (1,hid_dim)))\n",
    "    \n",
    "    h     = np.tanh(np.dot(Wx, x) + np.dot(Wh, h) + bh)\n",
    "      \n",
    "    # calcul de y à partir de h_t et des poids (Wy, by)\n",
    "    Wy    = np.transpose(w[\"dense\"][0])\n",
    "    by    = w[\"dense\"][1]\n",
    "    \n",
    "    y     = np.dot(Wy,h) + by\n",
    "       \n",
    "    return(y,h)\n",
    "\n",
    "# A partir de nos fonctions définies, nous pouvons calculer les sorties pour l'ensemble des batches donnés\n",
    "# en entrée\n",
    "\n",
    "# 1) Dans le premier exemple, on prédit les données indépendemment\n",
    "# - On réinitialise h pour chaque batch\n",
    "# [- L'entrée pour le calcul à l'instant t est la séquence du batch de l'instant t \n",
    "# (ne dépend pas de la sortie du batch précédent)]\n",
    "xt = []\n",
    "yt, ht = [], []\n",
    "ht.append(np.zeros((hid_dim,1))) # (mode : stateful=False)\n",
    "\n",
    "# On initialise la première entrée\n",
    "xt.append(x[0,1,0])\n",
    "for i in range(x.shape[1]):\n",
    "    \n",
    "    y, h = RNN_predict(w,xt[-1],ht[-1])\n",
    "    \n",
    "    # On conserve la sortie et l'état de la mémoire\n",
    "    # On recopie la sortie prédite dans l'entrée suivante\n",
    "    xt.append(y)\n",
    "    yt.append(y)\n",
    "    ht.append(h)\n",
    "\n",
    "yt=np.reshape(np.array(yt), (len(yt),1))\n",
    "\n",
    "# On trace les x et les y prédits\n",
    "plt.plot(x.flatten(), label='True data')\n",
    "plt.plot(yt, label='Predicted data')\n",
    "plt.legend()\n",
    "plt.xlim(0,250)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
