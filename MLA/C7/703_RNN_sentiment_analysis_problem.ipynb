{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/fr/8/81/Sciences_SU.png\" width=\"240\" height=\"240\" align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLA 703. RNN-LSTM et architectures avancées [Analyse de sentiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dans ce notebook, nous allons nous intéresser à des tâches d'analyse de sentiments\n",
    "# -> c'est à dire prédire un label de sentiment (ici positif ou négatif) à partir d'un texte\n",
    "\n",
    "# Ce notebook vise à approfondir : \n",
    "# - L'application du DL sur des données textuelles\n",
    "# - La compréhension des architectures RNN avancées comme les LSTM et les mécanismes d'attention\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importation des modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On importe les librairies usuelless\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# On désactive les warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Charger les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Formater/Préparer les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On charge directement la base IMDB par les datasets de Keras\n",
    "# La méthode load_date possède pas mal d'options et de propriétés intéressantes (voir description) :\n",
    "# 1) Sépare les jeux d'entrainement et de test\n",
    "# 2) num_words : Top most frequent words to consider.\n",
    "# 3) skip_top : Top most frequent words to ignore (they will appear as oov_char value in the sequence data).\n",
    "# 4) maxlen : Maximum sequence length. Any longer sequence will be truncated.\n",
    "# 5) seed : Seed for reproducible data shuffling.\n",
    "# 6) start_char : The start of a sequence will be marked with this character. Set to 1 because 0 is usually the padding character.\n",
    "# 7) oov_char : words that were cut out because of the num_words or skip_top limit will be replaced with this character.\n",
    "# 8) index_from : Index actual words with this index and higher.\n",
    "\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "num_words = 5000\n",
    "max_len   =  100\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words,\n",
    "                                                      maxlen=max_len)\n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La phrase avant padding est : \n",
      " [1, 778, 128, 74, 12, 630, 163, 15, 4, 1766, 2, 1051, 2, 32, 85, 156, 45, 40, 148, 139, 121, 664, 665, 10, 10, 1361, 173, 4, 749, 2, 16, 3804, 8, 4, 226, 65, 12, 43, 127, 24, 2, 10, 10]\n",
      "La phrase paddée sur une longueur 100 est : \n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    1  778  128   74   12  630  163   15    4 1766    2 1051    2\n",
      "   32   85  156   45   40  148  139  121  664  665   10   10 1361  173\n",
      "    4  749    2   16 3804    8    4  226   65   12   43  127   24    2\n",
      "   10   10]\n",
      "2773\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# On padde les séquences\n",
    "\n",
    "print(\"La phrase avant padding est : \\n {}\". format(x_train[0]))\n",
    "\n",
    "# On padde les séquence de mot\n",
    "max_len       = 100\n",
    "x_train       = pad_sequences(x_train, maxlen=max_len, truncating='post')\n",
    "x_test        = pad_sequences(x_test, maxlen=max_len, truncating='post')\n",
    "\n",
    "print(\"La phrase paddée sur une longueur {} est : \\n {}\". format(max_len, x_train[0]))\n",
    "      \n",
    "print(len(x_train))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Déclaration du réseau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tâche consiste à prédire la valence d'un texte à partir de son contenu.\n",
    "Pour ce faire nous allons créer une architecture many-to-one \n",
    "avec les réseaux RNNs\n",
    "Le problème consiste à implémenter et apprendre\n",
    "des réseaux avec - par exemple - les configurations suivantes : \n",
    "On va comparer les configurations suivantes : \n",
    "- Simple RNN avec dropout (avec un taux de 25% en sortie du RNN)\n",
    "- RNN-LSTM gauche-droite classique\n",
    "- LSTM bi-directionnel et return_sequences = False\n",
    "- Plusieurs couches au choix. On veillera en particulier \n",
    "  à la valeur des arguments return_sequences\n",
    "  \n",
    "Dans le rapport à faire directement dans le notebook, on prendra soin de :\n",
    "- reporter les losses sur les ensembles d'entrainement et de validation\n",
    "- mesurer l'accuracy sur l'ensemble de test\n",
    "On commentera les résultats obtenus en comparant les configurations\n",
    "Quelle est la configuration donnant la meilleure performance ? \n",
    "Pourquoi ?\n",
    "\n",
    "Aide : une couche LSTM bi-directionnelle est obtenue en appliquant sur la même couche une couche LSTM et une couche Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 128)         640000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 64)                12352     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 652,417\n",
      "Trainable params: 652,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# On importe les librairies pour le RNN\n",
    "from tensorflow.keras.layers import Dense , Input , SimpleRNN, LSTM , Embedding, Dropout\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Convolution1D\n",
    "#from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "embed_size = 128                                                   # dimension de l'embedding\n",
    "RNN_size   = 64\n",
    "\n",
    "# Example d'architecture pour le réseau RNN simple avec dropout\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embed_size))                 # layer embedding\n",
    "model.add(SimpleRNN(RNN_size, return_sequences = False))    # layer RNN\n",
    "model.add(Dropout(0.25))                                    # layer Dropout\n",
    "model.add(Dense(1))                                         # layer Dense\n",
    "\n",
    "# On affiche l'architecture de notre modèle\n",
    "model.summary()\n",
    "\n",
    "# On spécifie la fonction de perte, l'optimiseur, et la fonction d'évaluation\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entrainement du réseau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "35/35 [==============================] - 2s 43ms/step - loss: 1.6740 - accuracy: 0.4986 - val_loss: 0.7376 - val_accuracy: 0.5658\n",
      "Epoch 2/10\n",
      "35/35 [==============================] - 1s 37ms/step - loss: 0.8188 - accuracy: 0.6456 - val_loss: 1.0739 - val_accuracy: 0.5694\n",
      "Epoch 3/10\n",
      "35/35 [==============================] - 1s 40ms/step - loss: 0.5769 - accuracy: 0.7683 - val_loss: 0.8825 - val_accuracy: 0.5820\n",
      "Epoch 4/10\n",
      "35/35 [==============================] - 1s 37ms/step - loss: 0.1870 - accuracy: 0.9396 - val_loss: 1.1936 - val_accuracy: 0.6072\n",
      "Epoch 5/10\n",
      "35/35 [==============================] - 1s 36ms/step - loss: 0.0559 - accuracy: 0.9865 - val_loss: 1.6935 - val_accuracy: 0.6054\n",
      "Epoch 6/10\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.0253 - accuracy: 0.9986 - val_loss: 2.3846 - val_accuracy: 0.5928\n",
      "Epoch 7/10\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.0085 - accuracy: 0.9995 - val_loss: 2.3912 - val_accuracy: 0.5928\n",
      "Epoch 8/10\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 2.5719 - val_accuracy: 0.5964\n",
      "Epoch 9/10\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 2.6572 - val_accuracy: 0.5802\n",
      "Epoch 10/10\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 2.7880 - val_accuracy: 0.5802\n"
     ]
    }
   ],
   "source": [
    "# On entraine le réseau\n",
    "batch_size = 64                                                             # tailles des mini-batch\n",
    "epochs = 10                                                                 # nombre d'époques\n",
    "history = model.fit(x_train,y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2) # on entraine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On trace la loss et l'accuracy du modèle\n",
    "# On trace l'évolution de l'accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy']) \n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# On trace l'évolution de la loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history\n",
    "\n",
    "# On prédit sur l'ensemble de test\n",
    "\n",
    "# On prédit sur les données de test\n",
    "y_hat = model.predict(x_test)\n",
    "\n",
    "# On tranforme les prédictions en labels\n",
    "i_pos = [i for i in range(len(y_hat)) if y_hat[i]>0]\n",
    "i_neg = [i for i in range(len(y_hat)) if y_hat[i]<=0]\n",
    "\n",
    "y_pred   = np.zeros(len(y_hat))\n",
    "y_pred[i_pos] = 1\n",
    "y_pred[i_neg] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On importe les librairies pour l'évaluation\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# On calcule la matrice de confusion\n",
    "cm_test = confusion_matrix(y_test, y_pred)\n",
    "print('La matrice de confusion sur le jeu de test :\\n', cm_test, '\\n')\n",
    "\n",
    "# On calcul le score d accuracy\n",
    "acc_train=accuracy_score(y_test, y_pred)\n",
    "print('L accuracy sur le jeu de test est :\\n', acc_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
