{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFJVe1foDhWy"
   },
   "source": [
    "# UE AI for robotics: Practials on Evolutionary Algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVbRqn8MDhW2"
   },
   "source": [
    "* Student1: Name: XXX Surname: XXX\n",
    "* Student2: Name: XXX Surname: XXX\n",
    "\n",
    "\n",
    "To facilitate the corrections, we kindly ask each student to send the project on their own.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUJFd_eSDhW6"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this practical you will have the opportunity to test different multi-objective evolutionary algorithms and observe how they behave. For this purpose, different functions are provided to trace the individuals generated at each generation or to compare the results between them. You will also test [Gym](https://www.gymlibrary.dev/) (in its new version provided by the [gymnasium package](https://pypi.org/project/Gymnasium/), an environment used in reinforcement learning to conduct standardized learning experiments on (virtual) robots.\n",
    "\n",
    "To get a basic understanding of evolutionary algorithms you are required to complete this notebook and submit it on Moodle.\n",
    "\n",
    "**You are asked to ensure the readability of your notebook**: limit the display to what is necessary and don't forget to remove displays that you may use during the development and debugging phase. The readability of your notebook will affect the evaluation of your work.\n",
    "\n",
    "You will need the [DEAP library](https://deap.readthedocs.io/en/master/) and [Gymnasium](https://github.com/Farama-Foundation/Gymnasium). These libraries are easily installed with the `pip` command, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xottcHtQDhW7",
    "outputId": "29abab5e-e2f1-4a25-a945-40d87d5e2f8b"
   },
   "outputs": [],
   "source": [
    "!pip install deap\n",
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FhZSLbXeDhW8"
   },
   "source": [
    "Note: you can also use the [SCOOP](https://github.com/soravux/scoop) (Scalable COncurrent Operations in Python) library, which allows you to parallelize the execution of your experiments and thus make the execution much faster. However, SCOOP cannot be used directly in a jupyter notebook. If you want to use it, you have to put your code in a python file and run it with a `python -m scoop my_code.py`. You are advised to start with the notebook. You will be able to use small population sizes and few generations. If you finish early, you can use SCOOP to run experiments with more varied population sizes and more generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15IM7bKxDhW9"
   },
   "outputs": [],
   "source": [
    "# Note: the import of a file is done only once. If you modify this file,\n",
    "# you have to restart your kernel if you want to take the changes into account.\n",
    "# You can avoid this in the following way:\n",
    "import importlib # one time only\n",
    "\n",
    "#import my_module_python # the module must have been imported once\n",
    "#importlib.reload(my_module_python) # this line allows to load the last version\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# so that the figures appear directly in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from deap import base, creator, benchmarks, algorithms\n",
    "\n",
    "import random\n",
    "from deap import tools\n",
    "\n",
    "\n",
    "import random\n",
    "from deap import tools\n",
    "\n",
    "# do not forget to initialize the random seed\n",
    "#The seed() method is used to initialize the random number generator.\n",
    "#The random number generator needs a number to start with (a seed value), to be able to generate a random number.\n",
    "random.seed()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lobMTvDHDhW-"
   },
   "source": [
    "## 1. Getting started with DEAP\n",
    "\n",
    "In the rest of the practical, you will be asked to use [DEAP](https://deap.readthedocs.io/en/master/index.html), which is an open source python library.\n",
    "\n",
    "DEAP is a library for rapid prototyping of evolutionary algorithms. It allows to work at several levels, from the complete implementation of the algorithm from basic bricks to the black-box use of a fully implemented algorithm and through intermediate approaches, in which modules are reused, for example for selection, mutations or crossovers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSFEtcy8DhW-"
   },
   "source": [
    "\n",
    "Write an elitist evolutionary algorithm that will generate lambda parameter vector individuals and choose the best `mu` from the parents and children.\n",
    "\n",
    "You will use a SBX ([Simulated Binary Crossover](https://wpmedia.wolfram.com/uploads/sites/13/2018/02/09-2-2.pdf)) type crossover and a bounded polynomial type mutation (`eta=15.0`).\n",
    "\n",
    "The parameters will be between `-5` and `5` (you can use `random.uniform` for the initialization).\n",
    "\n",
    "You will use the functions of the DEAP toolbox to make your implementation easier and you will be able to draw inspiration from the examples provided.\n",
    "\n",
    "You will test your algorithm on the [Ackley function](https://machinelearningmastery.com/differential-evolution-global-optimization-with-python/#:~:text=The%20Ackley%20function%20is%20an,%5D%2C%20which%20evaluates%20to%200.0.). It is available in DEAP and can be called in any dimension. You can do your tests with dimension `10`, for example.\n",
    "\n",
    "\n",
    "Evolutionary algorithms have a stochastic part and therefore do not always give the same result. You will therefore plot the evolution of the average fitness for 10 independent runs. Instead of plotting the `10` curves, you will plot the median of the averages and an interval representing the 1st and 3rd quartile. You will do this as follows:\n",
    "\n",
    "    plt.plot(gen,means, label=\"Median of the average fitnesses\")\n",
    "    plt.fill_between(gen, fit_25, fit_75, alpha=0.25, linewidth=0)\n",
    "\n",
    "`gen` being a list of generations, `means` the list of medians of means and `fit_25` and `fit_75` the 1st and 3rd quartiles for these generations. You can determine `fit_25` and `fit_75` with the quantile function of numpy: `quantile(points, 0.25)` and `quantile(points, 0.75)`, with points a list that contains the fitness of the different runs at a given generation (so you have to loop and call these functions for each generation).\n",
    "\n",
    "\n",
    "Draw the curves for populations of increasing size: [5, 10, 100, 200]. What do you notice?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nePHnHYSDhW_"
   },
   "outputs": [],
   "source": [
    "# parameters values\n",
    "MIN_VALUE = -5\n",
    "MAX_VALUE = 5\n",
    "CXPB = 0.8 # Crossover rate\n",
    "MUTPB = 0.2 # Mutation rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aWoLMAMkDhW_"
   },
   "outputs": [],
   "source": [
    "# complete the code in this cell\n",
    "\n",
    "def ea_simple(n, nbgen, evaluate, IND_SIZE, weights=(-1.0,)):\n",
    "    \"\"\"Elitist evolutionary algorithm\n",
    "\n",
    "    Elitist evolutionary algorithm.\n",
    "    :param n: population size\n",
    "    :param nbgen: number of generations\n",
    "    :param evaluate: the evaluation function\n",
    "    :param IND_SIZE: the size of an individual\n",
    "    :param weights: the weights to use for the fitness (here it will be (-1.0,) for a function to minimize and (1.0,) for a function to maximize)\n",
    "    \"\"\"\n",
    "\n",
    "    if (hasattr(creator, \"FitnessMin\")):\n",
    "        del creator.FitnessMin\n",
    "    if (hasattr(creator, \"Individual\")):\n",
    "        del creator.Individual\n",
    "\n",
    "    creator.create(\"FitnessMin\", base.Fitness, weights=weights)\n",
    "    creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    ## to be completed to select the operators of mutation, crossing, selection with toolbox.register(...)\n",
    "\n",
    "\n",
    "    # Statistics to retrieve the results\n",
    "    stats = tools.Statistics(key=lambda ind: ind.fitness.values)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"std\", np.std)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    # The structure that allows to store the statistics\n",
    "    logbook = tools.Logbook()\n",
    "\n",
    "    # The structure to recover the best individual\n",
    "    hof = tools.HallOfFame(1)\n",
    "\n",
    "    ## to complete to initialize the algorithm, don't forget to update the statistics, the logbook and the hall-of-fame.\n",
    "\n",
    "\n",
    "    for gen in range(1, nbgen):\n",
    "\n",
    "        # To see the progress\n",
    "        if (g%10==0):\n",
    "            print(\"+\",end=\"\", flush=True)\n",
    "        else:\n",
    "            print(\".\",end=\"\", flush=True)\n",
    "\n",
    "\n",
    "        ## to complete by not forgetting to update the statistics, the logbook and the hall-of-fame\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return pop, hof, logbook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Ym_6XHWDhXA"
   },
   "outputs": [],
   "source": [
    "# Insert here the code to call your algorithm and draw the result curves\n",
    "\n",
    "pop_size = [5, 10, 100, 200]\n",
    "nbgen = 500 # Insert here the code to call your algorithm and draw the result curves\n",
    "ind_size = 10\n",
    "nbrun = 10\n",
    "\n",
    "## to complete, don't forget to put some comments on the results in the next cell\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N51Y6wtqDhXB"
   },
   "source": [
    "** Comments on the results**: to be completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MFyRDW-DhXB"
   },
   "source": [
    "## 2. Policy learning with gym\n",
    "\n",
    "The objective of this question is to do to learn the first policies. You will use a very simple problem: the inverted pendulum. It consists in controlling a pendulum fixed on a cart and able to rotate. The cart can move horizontally with two actions ([\"bang-bang\" control](https://en.wikipedia.org/wiki/Bang%E2%80%93bang_control)). The horizontal movements of the pendulum cause the pendulum to swing and the goal is to keep it vertical. You will use a neural network type policy. The corresponding code is provided in the appendix (so you have to run it to access the corresponding functions).\n",
    "\n",
    "OpenAI-gym (now Gymnasium) is a framework for implementing reinforcement learning experiments. It offers a simple and unified interface and includes many environments used to test reinforcement learning algorithms. You will use this environment and its module [CartPole-v1](https://gymnasium.farama.org/environments/classic_control/cart_pole/).\n",
    "\n",
    "Complete the code below. You will plot the results of the fitness as in the previous question. The calculations take more time, so do them according to your computational capacities and, if necessary, indicate in the comments what you would expect to observe with a higher computational power. You can do the plot only for one population size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SnVSbxixDhXB"
   },
   "outputs": [],
   "source": [
    "# to complete to call your evolutionary algorithm on the different population sizes and plot the evolution of the average fitness\n",
    "# to comment the results in a few words.\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "nn=SimpleNeuralControllerNumpy(4,1,2,5)\n",
    "IND_SIZE=len(nn.get_parameters())\n",
    "nn.init_random_params()\n",
    "random_genotype = nn.get_parameters()\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "\"\"\"\n",
    "Comments:\n",
    "\n",
    "the number of weights corresponds to the number of connections\n",
    "the number of bias corresponds to the number of neurons\n",
    "genotype is an array containing weights and biases\n",
    "\n",
    "\"\"\"\n",
    "def eval_nn(genotype, render=False, nbstep=500):\n",
    "    total_reward=0\n",
    "    nn.set_parameters(genotype)\n",
    "\n",
    "    # nbstep is the number of time steps. The larger it is, the more stable your pendulum will be, but on the other hand, the longer your calculations will take. You can therefore adjust this\n",
    "    # value to speed up or slow down your calculations. Use the default value to indicate what should happen during the training, you can indicate a\n",
    "    # value to visualize the behavior of the result obtained.\n",
    "\n",
    "    # use render to activate or inhibit the display (if not notebook).\n",
    "\n",
    "    ## to be completed according to the CartPole environment documentation\n",
    "\n",
    "\n",
    "    return total_reward,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQ5BM2o5DhXC"
   },
   "outputs": [],
   "source": [
    "pop_size = [10]\n",
    "nbgen = 50\n",
    "nbrun = 2\n",
    "\n",
    "## to complete... Don't forget to put some comments on the results obtained in the next cell.\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOow0rB9DhXC"
   },
   "source": [
    "** Comments on the results**: to be completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOXQ3JKDDhXC"
   },
   "source": [
    "Note: The pendulum evaluation starts at a random position. Display the obtained fitness several times in a row by viewing the same individual. You should observe that the fitness obtained is not always the same. This is an illustration of the generalization problem: during learning, a policy has only been tested under a particular condition. If you change the conditions a bit, you have no guarantee of what will happen... To limit this problem, a simple strategy is to calculate the fitness not on a single evaluation, but on several. This should reduce the observed variability. If your computing power allows it, you can modify your evaluation function this way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocpc9urlDhXD"
   },
   "source": [
    "## 3 Implementing NSGA-II\n",
    "\n",
    "Now implement NSGA-II based on the functions provided in DEAP. You will test NSGA-II on a multi-objective benchmark provided in DEAP, for example, the [Fonseca and Fleming function](http://www.mathlayer.com/support/benchmark-problems-fonseca-fleming.html).\n",
    "\n",
    "It is not immediate to characterize the performance of a multi-objective algorithm with a one-dimensional indicator. You will use the hypervolume which corresponds to the volume bounded by the pareto front and a given reference point.\n",
    "\n",
    "As before, plot the evolution of the hypervolume (median and 1st and 3rd quartiles) for populations of size [5, 10, 100, 200].\n",
    "\n",
    "What do you notice?\n",
    "\n",
    "In the case of the Fonseca function, you can use (1,1) as a reference point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MuNwmdmkDhXD"
   },
   "outputs": [],
   "source": [
    "print(\"Example of use of the hypervolume calculation code. The hypothesis is that of a minimization.\")\n",
    "print(\"A reference point should be given corresponding to, for example, the maximum values for the objectives.\")\n",
    "from deap.tools._hypervolume import hv\n",
    "print(\"Hypervolume: %f\"%(hv.hypervolume([np.array([1,0]), np.array([1,1]), np.array([0,1])], np.array([2,2]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6cGIrYy-DhXD"
   },
   "outputs": [],
   "source": [
    "BOUND_LOW, BOUND_UP = -5, 5\n",
    "CXPB = 0.9\n",
    "MUTPB = 0.2\n",
    "\n",
    "def my_nsga2(n, nbgen, evaluate, ref_point=np.array([1,1]), IND_SIZE=5, weights=(-1.0, -1.0)):\n",
    "    \"\"\"NSGA-2\n",
    "\n",
    "    NSGA-2\n",
    "    :param n: population size\n",
    "    :param nbgen: number of generation\n",
    "    :param evaluate: the evaluation function\n",
    "    :param ref_point: the reference point for the calculation of the hypervolume\n",
    "    :param IND_SIZE: the size of an individual\n",
    "    :param weights: the weights to use for the fitness (here it will be (-1.0,) for a function to minimize and (1.0,) for a function to maximize)\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    NSGA-2 Main Loop:\n",
    "\n",
    "    1) Create offspring and a combined population\n",
    "    2) Rank and sort offspring due to performance* on defined target indicators\n",
    "    3) Take best members to create new population including a good spread in solutions\n",
    "\n",
    "    * the performance here is given by the hypervolume\n",
    "    \"\"\"\n",
    "    if (hasattr(creator, \"MaFitness\")):\n",
    "        del creator.MaFitness\n",
    "    if (hasattr(creator, \"Individual\")):\n",
    "        del creator.Individual\n",
    "    creator.create(\"MaFitness\", base.Fitness, weights=weights)\n",
    "    creator.create(\"Individual\", list, fitness=creator.MaFitness)\n",
    "\n",
    "\n",
    "    toolbox = base.Toolbox()\n",
    "    paretofront = tools.ParetoFront()\n",
    "\n",
    "    ## to complete (initialization, evaluation and update of the pareto front)\n",
    "\n",
    "    # To recover the hypervolume, we will just put the different values in a vector s_hv which will be returned by the function.\n",
    "    pointset=[np.array(ind.fitness.getValues()) for ind in paretofront]\n",
    "    s_hv=[hv.hypervolume(pointset, ref_point)]\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, nbgen):\n",
    "\n",
    "        if (gen%10==0):\n",
    "            print(\"+\",end=\"\", flush=True)\n",
    "        else:\n",
    "            print(\".\",end=\"\", flush=True)\n",
    "\n",
    "        ## to be completed (\"children\" population, evaluation, selection and update of the pareto front)\n",
    "\n",
    "\n",
    "        pointset=[np.array(ind.fitness.getValues()) for ind in paretofront]\n",
    "        s_hv.append(hv.hypervolume(pointset, ref_point))\n",
    "\n",
    "    return pop, paretofront, s_hv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bn9O9hz7DhXE"
   },
   "outputs": [],
   "source": [
    "random.seed()\n",
    "pop_size = [5, 10, 100, 200]\n",
    "nbgenn = 100 # to reduce at first if too slow\n",
    "\n",
    "## to complete. Do not forget to put a comment on the results in the next cell.\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugPCAh4pDhXE"
   },
   "source": [
    "** Comments on the results**: to be completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVY2YXPdDhXE"
   },
   "source": [
    "## 4. Direct learning in the policy space, multi-objective version\n",
    "\n",
    "The inverted pendulum is actually a multi-objective problem in which the pendulum must be kept vertical with the carriage centered in a given area.\n",
    "\n",
    "Complete the code below to minimize the error in x and theta with NSGA-2. You will be able to plot the evolution of the hypervolume or modify the code of NSGA2 to plot the evolution of the error in x and the error in theta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n51xxeezDhXF"
   },
   "outputs": [],
   "source": [
    "# to complete to call your evolutionary algorithm on the different population sizes and plot the evolution of the average fitness\n",
    "# to comment the results in a few words.\n",
    "\n",
    "\n",
    "def eval_nn2(genotype, render = False, nbstep=500):\n",
    "    \"\"\"\n",
    "    An observation gives us the following information:\n",
    "\n",
    "    idx | info\n",
    "    ------------------\n",
    "    0\t| Cart Position\n",
    "    1\t| Cart Velocity\n",
    "    2\t| Pole Angle\n",
    "    3\t| Pole Velocity At Tip\n",
    "\n",
    "    \"\"\"\n",
    "    total_pos = 0 # the error in x is in observation[0]\n",
    "    total_angle = 0 # the error in theta is in obervation [2]\n",
    "    nn=SimpleNeuralControllerNumpy(4,1,2,5)\n",
    "    nn.set_parameters(genotype)\n",
    "\n",
    "    obs, info = env.reset(seed=42)\n",
    "\n",
    "    ## to complete ...\n",
    "\n",
    "    # ATTENTION: you are in the case of a fitness to minimize. Interrupting the evaluation\n",
    "    # as soon as possible is a strategy that the evolutionary algorithm\n",
    "    # algorithm can use to minimize the fitness. In the case where the pendulum falls before the end,\n",
    "    # it is therefore necessary to add to the fitness a value that will guide the learning towards the right\n",
    "    # behaviors. You can for example add n times a penalty, n being the number of time steps left. This will push the algorithm to minimize the penalty and thus avoid the fall. The penalty can be the error at the time of the fall or the maximum error.\n",
    "\n",
    "    return (total_pos, total_angle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SBLmTGpjDhXF"
   },
   "outputs": [],
   "source": [
    "nn=SimpleNeuralControllerNumpy(4,1,2,5)\n",
    "IND_SIZE=len(nn.get_parameters())\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "random.seed()\n",
    "pop_size = [10]\n",
    "nbgenn = 50\n",
    "\n",
    "\n",
    "## to complete. Do not forget to fill in the next cell with comments on the results\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPKrzQ_2DhXF"
   },
   "source": [
    "** Comments on the results**: to be completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQlEJU0pDhXG"
   },
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpSIm9keDhXG"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1./(1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def gen_simplemlp(n_in, n_out, n_hidden_layers=2, n_neurons_per_hidden=5):\n",
    "    n_neurons = [n_neurons_per_hidden]*n_hidden_layers if np.isscalar(n_neurons_per_hidden) else n_neurons_per_hidden\n",
    "    i = Input(shape=(n_in,))\n",
    "    x = i\n",
    "    for n in n_neurons:\n",
    "        x = Dense(n, activation='sigmoid')(x)\n",
    "    o = Dense(n_out, activation='tanh')(x)\n",
    "    m = Model(inputs=i, outputs=o)\n",
    "    return m\n",
    "\n",
    "\n",
    "class SimpleNeuralControllerNumpy():\n",
    "    def __init__(self, n_in, n_out, n_hidden_layers=2, n_neurons_per_hidden=5, params=None):\n",
    "        self.dim_in = n_in\n",
    "        self.dim_out = n_out\n",
    "        # if params is provided, we look for the number of hidden layers and neuron per layer into that parameter (a dicttionary)\n",
    "        if (not params==None):\n",
    "            if (\"n_hidden_layers\" in params.keys()):\n",
    "                n_hidden_layers=params[\"n_hidden_layers\"]\n",
    "            if (\"n_neurons_per_hidden\" in params.keys()):\n",
    "                n_neurons_per_hidden=params[\"n_neurons_per_hidden\"]\n",
    "        self.n_per_hidden = n_neurons_per_hidden\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.weights = None\n",
    "        self.n_weights = None\n",
    "        self.init_random_params()\n",
    "        self.out = np.zeros(n_out)\n",
    "        #print(\"Creating a simple mlp with %d inputs, %d outputs, %d hidden layers and %d neurons per layer\"%(n_in, n_out,n_hidden_layers, n_neurons_per_hidden))\n",
    "\n",
    "\n",
    "    def init_random_params(self):\n",
    "        if(self.n_hidden_layers > 0):\n",
    "            self.weights = [np.random.random((self.dim_in,self.n_per_hidden))] # In -> first hidden\n",
    "            self.bias = [np.random.random(self.n_per_hidden)] # In -> first hidden\n",
    "            for i in range(self.n_hidden_layers-1): # Hidden -> hidden\n",
    "                self.weights.append(np.random.random((self.n_per_hidden,self.n_per_hidden)))\n",
    "                self.bias.append(np.random.random(self.n_per_hidden))\n",
    "            self.weights.append(np.random.random((self.n_per_hidden,self.dim_out))) # -> last hidden -> out\n",
    "            self.bias.append(np.random.random(self.dim_out))\n",
    "        else:\n",
    "            self.weights = [np.random.random((self.dim_in,self.dim_out))] # Single-layer perceptron\n",
    "            self.bias = [np.random.random(self.dim_out)]\n",
    "        self.n_weights = np.sum([np.product(w.shape) for w in self.weights]) + np.sum([np.product(b.shape) for b in self.bias])\n",
    "\n",
    "    def get_parameters(self):\n",
    "        \"\"\"\n",
    "        Returns all network parameters as a single array\n",
    "        \"\"\"\n",
    "        flat_weights = np.hstack([arr.flatten() for arr in (self.weights+self.bias)])\n",
    "        return flat_weights\n",
    "\n",
    "    def set_parameters(self, flat_parameters):\n",
    "        \"\"\"\n",
    "        Set all network parameters from a single array\n",
    "        \"\"\"\n",
    "        i = 0 # index\n",
    "        to_set = []\n",
    "        self.weights = list()\n",
    "        self.bias = list()\n",
    "        if(self.n_hidden_layers > 0):\n",
    "            # In -> first hidden\n",
    "            w0 = np.array(flat_parameters[i:(i+self.dim_in*self.n_per_hidden)])\n",
    "            self.weights.append(w0.reshape(self.dim_in,self.n_per_hidden))\n",
    "            i += self.dim_in*self.n_per_hidden\n",
    "            for l in range(self.n_hidden_layers-1): # Hidden -> hidden\n",
    "                w = np.array(flat_parameters[i:(i+self.n_per_hidden*self.n_per_hidden)])\n",
    "                self.weights.append(w.reshape((self.n_per_hidden,self.n_per_hidden)))\n",
    "                i += self.n_per_hidden*self.n_per_hidden\n",
    "            # -> last hidden -> out\n",
    "            wN = np.array(flat_parameters[i:(i+self.n_per_hidden*self.dim_out)])\n",
    "            self.weights.append(wN.reshape((self.n_per_hidden,self.dim_out)))\n",
    "            i += self.n_per_hidden*self.dim_out\n",
    "            # Samefor bias now\n",
    "            # In -> first hidden\n",
    "            b0 = np.array(flat_parameters[i:(i+self.n_per_hidden)])\n",
    "            self.bias.append(b0)\n",
    "            i += self.n_per_hidden\n",
    "            for l in range(self.n_hidden_layers-1): # Hidden -> hidden\n",
    "                b = np.array(flat_parameters[i:(i+self.n_per_hidden)])\n",
    "                self.bias.append(b)\n",
    "                i += self.n_per_hidden\n",
    "            # -> last hidden -> out\n",
    "            bN = np.array(flat_parameters[i:(i+self.dim_out)])\n",
    "            self.bias.append(bN)\n",
    "            i += self.dim_out\n",
    "        else:\n",
    "            n_w = self.dim_in*self.dim_out\n",
    "            w = np.array(flat_parameters[:n_w])\n",
    "            self.weights = [w.reshape((self.dim_in,self.dim_out))]\n",
    "            self.bias = [np.array(flat_parameters[n_w:])]\n",
    "        self.n_weights = np.sum([np.product(w.shape) for w in self.weights]) + np.sum([np.product(b.shape) for b in self.bias])\n",
    "\n",
    "    def predict(self,x):\n",
    "        \"\"\"\n",
    "        Propagage\n",
    "        \"\"\"\n",
    "        if(self.n_hidden_layers > 0):\n",
    "            #Input\n",
    "            a = np.matmul(x,self.weights[0]) + self.bias[0]\n",
    "            y = sigmoid(a)\n",
    "            # hidden -> hidden\n",
    "            for i in range(1,self.n_hidden_layers-1):\n",
    "                a = np.matmul(y, self.weights[i]) + self.bias[i]\n",
    "                y = sigmoid(a)\n",
    "            # Out\n",
    "            a = np.matmul(y, self.weights[-1]) + self.bias[-1]\n",
    "            out = tanh(a)\n",
    "            return out\n",
    "        else: # Simple monolayer perceptron\n",
    "            return tanh(np.matmul(x,self.weights[0]) + self.bias[0])\n",
    "\n",
    "    def __call__(self,x):\n",
    "        \"\"\"Calling the controller calls predict\"\"\"\n",
    "        return self.predict(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clu3euZMDhXG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
