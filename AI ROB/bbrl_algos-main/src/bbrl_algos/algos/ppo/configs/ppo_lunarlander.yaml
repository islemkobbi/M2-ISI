    save_best: True
    plot_agents: False
    collect_stats: True

    logger:
      classname: bbrl.utils.logger.TFLogger
      log_dir: ./ppo_logs/
      verbose: False
      every_n_seconds: 10

    algorithm:
      seed:
        train: 1
        eval: 99
        policy: 123
        torch: 789

      max_grad_norm: 0.5
      n_envs: 10
      n_steps_train: 50
      eval_interval: 10000
      nb_evals: 10
      gae: 0.8
      n_steps: 900_000
      beta: 0.01
      discount_factor: 0.9
      clip_range: 0.2
      clip_range_vf: 0
      entropy_coef: 0.0
      critic_coef: 0.4
      policy_coef: 1
      opt_epochs: 3
      batch_size: 16
      policy_type: DiscretePPOActor
      architecture:
        policy_hidden_size: [128, 128]
        critic_hidden_size: [512, 512]

    gym_env:
      env_name: LunarLander-v2

    optimizer:
      classname: torch.optim.Adam
      lr: 0.005