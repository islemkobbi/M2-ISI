{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[download this notebook here](https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/2_train_dagger.ipynb)\n",
    "# Train an Agent using the DAgger Algorithm\n",
    "\n",
    "The DAgger algorithm is an extension of behavior cloning. \n",
    "In behavior cloning, the training trajectories are recorded directly from an expert.\n",
    "In DAgger, the learner generates the trajectories but an expert corrects the actions with the optimal actions in each of the visited states.\n",
    "This ensures that the state distribution of the training data matches that of the learner's current policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need an expert to learn from. For convenience we download one from the HuggingFace model hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kobbi\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:166: UserWarning: Could not deserialize object learning_rate. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code() argument 13 must be str, not int\n",
      "  warnings.warn(\n",
      "c:\\Users\\kobbi\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:166: UserWarning: Could not deserialize object clip_range. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code() argument 13 must be str, not int\n",
      "  warnings.warn(\n",
      "c:\\Users\\kobbi\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:166: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code() argument 13 must be str, not int\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from imitation.policies.serialize import load_policy\n",
    "from imitation.util.util import make_vec_env\n",
    "import numpy as np\n",
    "\n",
    "env = make_vec_env(\n",
    "    \"seals:seals/CartPole-v0\",\n",
    "    rng=np.random.default_rng(),\n",
    "    n_envs=1,\n",
    ")\n",
    "expert = load_policy(\n",
    "    \"ppo-huggingface\",\n",
    "    organization=\"HumanCompatibleAI\",\n",
    "    env_name=\"seals/CartPole-v0\",\n",
    "    venv=env,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyWrapper:\n",
    "    def __init__(self, original_policy, env, acc):\n",
    "        self.original_policy = original_policy\n",
    "        self.env = env\n",
    "        self.acc = acc\n",
    "\n",
    "    def predict(self, *args,):\n",
    "        if np.random.rand() < self.acc:\n",
    "            return self.original_policy.predict(*args)\n",
    "\n",
    "        else:\n",
    "            x = self.original_policy.predict(*args)\n",
    "            x1 = 1-x[0]\n",
    "            out = (x1, x[1])\n",
    "            print(out)\n",
    "            return self.env.action_space.sample()\n",
    "       \n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.original_policy, name)\n",
    "\n",
    "\n",
    "wrapped_policy = PolicyWrapper(expert, env, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can construct a DAgger trainer und use it to train the policy on the cartpole environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kobbi\\AppData\\Local\\Temp\\dagger_example_ohfh7mon\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Policy must be None, a stable-baselines policy or algorithm, or a Callable, got <class '__main__.PolicyWrapper'> instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 21\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(tmpdir)\n\u001b[0;32m     14\u001b[0m dagger_trainer \u001b[38;5;241m=\u001b[39m SimpleDAggerTrainer(\n\u001b[0;32m     15\u001b[0m     venv\u001b[38;5;241m=\u001b[39menv,\n\u001b[0;32m     16\u001b[0m     scratch_dir\u001b[38;5;241m=\u001b[39mtmpdir,\n\u001b[0;32m     17\u001b[0m     expert_policy\u001b[38;5;241m=\u001b[39mwrapped_policy,\n\u001b[0;32m     18\u001b[0m     bc_trainer\u001b[38;5;241m=\u001b[39mbc_trainer,\n\u001b[0;32m     19\u001b[0m     rng\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mdefault_rng())\n\u001b[1;32m---> 21\u001b[0m dagger_trainer\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m2000\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kobbi\\anaconda3\\Lib\\site-packages\\imitation\\algorithms\\dagger.py:669\u001b[0m, in \u001b[0;36mSimpleDAggerTrainer.train\u001b[1;34m(self, total_timesteps, rollout_round_min_episodes, rollout_round_min_timesteps, bc_train_kwargs)\u001b[0m\n\u001b[0;32m    662\u001b[0m round_timestep_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    664\u001b[0m sample_until \u001b[38;5;241m=\u001b[39m rollout\u001b[38;5;241m.\u001b[39mmake_sample_until(\n\u001b[0;32m    665\u001b[0m     min_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(rollout_round_min_timesteps, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size),\n\u001b[0;32m    666\u001b[0m     min_episodes\u001b[38;5;241m=\u001b[39mrollout_round_min_episodes,\n\u001b[0;32m    667\u001b[0m )\n\u001b[1;32m--> 669\u001b[0m trajectories \u001b[38;5;241m=\u001b[39m rollout\u001b[38;5;241m.\u001b[39mgenerate_trajectories(\n\u001b[0;32m    670\u001b[0m     policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpert_policy,\n\u001b[0;32m    671\u001b[0m     venv\u001b[38;5;241m=\u001b[39mcollector,\n\u001b[0;32m    672\u001b[0m     sample_until\u001b[38;5;241m=\u001b[39msample_until,\n\u001b[0;32m    673\u001b[0m     deterministic_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    674\u001b[0m     rng\u001b[38;5;241m=\u001b[39mcollector\u001b[38;5;241m.\u001b[39mrng,\n\u001b[0;32m    675\u001b[0m )\n\u001b[0;32m    677\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m traj \u001b[38;5;129;01min\u001b[39;00m trajectories:\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39mrecord_mean(\n\u001b[0;32m    679\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdagger/mean_episode_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    680\u001b[0m         np\u001b[38;5;241m.\u001b[39msum(traj\u001b[38;5;241m.\u001b[39mrews),\n\u001b[0;32m    681\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\kobbi\\anaconda3\\Lib\\site-packages\\imitation\\data\\rollout.py:412\u001b[0m, in \u001b[0;36mgenerate_trajectories\u001b[1;34m(policy, venv, sample_until, rng, deterministic_policy)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_trajectories\u001b[39m(\n\u001b[0;32m    383\u001b[0m     policy: AnyPolicy,\n\u001b[0;32m    384\u001b[0m     venv: VecEnv,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    388\u001b[0m     deterministic_policy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    389\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence[types\u001b[38;5;241m.\u001b[39mTrajectoryWithRew]:\n\u001b[0;32m    390\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate trajectory dictionaries from a policy and an environment.\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \n\u001b[0;32m    392\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;124;03m        should truncate if required.\u001b[39;00m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 412\u001b[0m     get_actions \u001b[38;5;241m=\u001b[39m policy_to_callable(policy, venv, deterministic_policy)\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;66;03m# Collect rollout tuples.\u001b[39;00m\n\u001b[0;32m    415\u001b[0m     trajectories \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\kobbi\\anaconda3\\Lib\\site-packages\\imitation\\data\\rollout.py:339\u001b[0m, in \u001b[0;36mpolicy_to_callable\u001b[1;34m(policy, venv, deterministic_policy)\u001b[0m\n\u001b[0;32m    336\u001b[0m     get_actions \u001b[38;5;241m=\u001b[39m policy\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 339\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    340\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPolicy must be None, a stable-baselines policy or algorithm, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    341\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Callable, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(policy)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    342\u001b[0m     )\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(policy, BaseAlgorithm):\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;66;03m# Check that the observation and action spaces of policy and environment match\u001b[39;00m\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: Policy must be None, a stable-baselines policy or algorithm, or a Callable, got <class '__main__.PolicyWrapper'> instead"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "\n",
    "from imitation.algorithms import bc\n",
    "from imitation.algorithms.dagger import SimpleDAggerTrainer\n",
    "\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    rng=np.random.default_rng(),\n",
    ")\n",
    "\n",
    "with tempfile.TemporaryDirectory(prefix=\"dagger_example_\") as tmpdir:\n",
    "    print(tmpdir)\n",
    "    dagger_trainer = SimpleDAggerTrainer(\n",
    "        venv=env,\n",
    "        scratch_dir=tmpdir,\n",
    "        expert_policy=wrapped_policy,\n",
    "        bc_trainer=bc_trainer,\n",
    "        rng=np.random.default_rng())\n",
    "\n",
    "    dagger_trainer.train(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the evaluation shows, that we actually trained a policy that solves the environment (500 is the max reward)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500.0\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "reward, _ = evaluate_policy(dagger_trainer.policy, env, 20)\n",
    "print(reward)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd378ce8f53beae712f05342da42c6a7612fc68b19bea03b52c7b1cdc8851b5f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
