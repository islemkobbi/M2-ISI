{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[download this notebook here](https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/2_train_dagger.ipynb)\n",
    "# Train an Agent using the DAgger Algorithm\n",
    "\n",
    "The DAgger algorithm is an extension of behavior cloning. \n",
    "In behavior cloning, the training trajectories are recorded directly from an expert.\n",
    "In DAgger, the learner generates the trajectories but an expert corrects the actions with the optimal actions in each of the visited states.\n",
    "This ensures that the state distribution of the training data matches that of the learner's current policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need an expert to learn from. For convenience we download one from the HuggingFace model hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\kobbi\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kobbi\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:166: UserWarning: Could not deserialize object learning_rate. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code() argument 13 must be str, not int\n",
      "  warnings.warn(\n",
      "c:\\Users\\kobbi\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:166: UserWarning: Could not deserialize object clip_range. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code() argument 13 must be str, not int\n",
      "  warnings.warn(\n",
      "c:\\Users\\kobbi\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:166: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code() argument 13 must be str, not int\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from imitation.policies.serialize import load_policy\n",
    "from imitation.util.util import make_vec_env\n",
    "\n",
    "env = make_vec_env(\n",
    "    \"seals:seals/CartPole-v0\",\n",
    "    rng=np.random.default_rng(),\n",
    "    n_envs=1,\n",
    ")\n",
    "expert = load_policy(\n",
    "    \"ppo-huggingface\",\n",
    "    organization=\"HumanCompatibleAI\",\n",
    "    env_name=\"seals/CartPole-v0\",\n",
    "    venv=env,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class n_expert():\n",
    "\n",
    "    def __init__(self,expert, n) -> None:\n",
    "\n",
    "        self.expert = expert\n",
    "        self.n = n\n",
    "\n",
    "\n",
    "    def act(self,obs, state, dones) :\n",
    "\n",
    "        if self.n < np.random.random() : \n",
    "            return self.expert(obs, state, dones) \n",
    "        else : \n",
    "            return 1 - self.expert(obs, state, dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_50 = n_expert(expert, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method n_expert.act of <__main__.n_expert object at 0x0000023BC3267810>>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expert_50.act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can construct a DAgger trainer und use it to train the policy on the cartpole environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kobbi\\AppData\\Local\\Temp\\dagger_example_lr65seuj\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ActorCriticPolicy.forward() takes from 2 to 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 22\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(tmpdir)\n\u001b[0;32m     14\u001b[0m dagger_trainer \u001b[38;5;241m=\u001b[39m SimpleDAggerTrainer(\n\u001b[0;32m     15\u001b[0m     venv\u001b[38;5;241m=\u001b[39menv,\n\u001b[0;32m     16\u001b[0m     scratch_dir\u001b[38;5;241m=\u001b[39mtmpdir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     rng\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mdefault_rng(),\n\u001b[0;32m     20\u001b[0m )\n\u001b[1;32m---> 22\u001b[0m dagger_trainer\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m2000\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kobbi\\Documents\\GitHub\\M2-ISI\\ApIHM\\projet\\dagger.py:671\u001b[0m, in \u001b[0;36mSimpleDAggerTrainer.train\u001b[1;34m(self, total_timesteps, rollout_round_min_episodes, rollout_round_min_timesteps, bc_train_kwargs)\u001b[0m\n\u001b[0;32m    664\u001b[0m round_timestep_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    666\u001b[0m sample_until \u001b[38;5;241m=\u001b[39m rollout\u001b[38;5;241m.\u001b[39mmake_sample_until(\n\u001b[0;32m    667\u001b[0m     min_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(rollout_round_min_timesteps, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size),\n\u001b[0;32m    668\u001b[0m     min_episodes\u001b[38;5;241m=\u001b[39mrollout_round_min_episodes,\n\u001b[0;32m    669\u001b[0m )\n\u001b[1;32m--> 671\u001b[0m trajectories \u001b[38;5;241m=\u001b[39m rollout\u001b[38;5;241m.\u001b[39mgenerate_trajectories(\n\u001b[0;32m    672\u001b[0m     policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpert_policy,\n\u001b[0;32m    673\u001b[0m     venv\u001b[38;5;241m=\u001b[39mcollector,\n\u001b[0;32m    674\u001b[0m     sample_until\u001b[38;5;241m=\u001b[39msample_until,\n\u001b[0;32m    675\u001b[0m     deterministic_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    676\u001b[0m     rng\u001b[38;5;241m=\u001b[39mcollector\u001b[38;5;241m.\u001b[39mrng,\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m traj \u001b[38;5;129;01min\u001b[39;00m trajectories:\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39mrecord_mean(\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdagger/mean_episode_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    682\u001b[0m         np\u001b[38;5;241m.\u001b[39msum(traj\u001b[38;5;241m.\u001b[39mrews),\n\u001b[0;32m    683\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\kobbi\\anaconda3\\Lib\\site-packages\\imitation\\data\\rollout.py:446\u001b[0m, in \u001b[0;36mgenerate_trajectories\u001b[1;34m(policy, venv, sample_until, rng, deterministic_policy)\u001b[0m\n\u001b[0;32m    443\u001b[0m dones \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(venv\u001b[38;5;241m.\u001b[39mnum_envs, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(active):\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;66;03m# policy gets unwrapped observations (eg as dict, not dictobs)\u001b[39;00m\n\u001b[1;32m--> 446\u001b[0m     acts, state \u001b[38;5;241m=\u001b[39m get_actions(obs, state, dones)\n\u001b[0;32m    447\u001b[0m     obs, rews, dones, infos \u001b[38;5;241m=\u001b[39m venv\u001b[38;5;241m.\u001b[39mstep(acts)\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    449\u001b[0m         obs,\n\u001b[0;32m    450\u001b[0m         (np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mdict\u001b[39m),\n\u001b[0;32m    451\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTuple observations are not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[13], line 12\u001b[0m, in \u001b[0;36mn_expert.act\u001b[1;34m(self, obs, state, dones)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mact\u001b[39m(\u001b[38;5;28mself\u001b[39m,obs, state, dones) :\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom() : \n\u001b[1;32m---> 12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpert(obs, state, dones) \n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m : \n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpert(obs, state, dones)\n",
      "File \u001b[1;32mc:\\Users\\kobbi\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kobbi\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: ActorCriticPolicy.forward() takes from 2 to 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "\n",
    "from imitation.algorithms import bc\n",
    "from dagger import SimpleDAggerTrainer\n",
    "\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    rng=np.random.default_rng(),\n",
    ")\n",
    "\n",
    "with tempfile.TemporaryDirectory(prefix=\"dagger_example_\") as tmpdir:\n",
    "    print(tmpdir)\n",
    "    dagger_trainer = SimpleDAggerTrainer(\n",
    "        venv=env,\n",
    "        scratch_dir=tmpdir,\n",
    "        expert_policy=expert_50.act,\n",
    "        bc_trainer=bc_trainer,\n",
    "        rng=np.random.default_rng())\n",
    "\n",
    "    dagger_trainer.train(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the evaluation shows, that we actually trained a policy that solves the environment (500 is the max reward)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500.0\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "reward, _ = evaluate_policy(dagger_trainer.policy, env, 20)\n",
    "print(reward)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd378ce8f53beae712f05342da42c6a7612fc68b19bea03b52c7b1cdc8851b5f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
